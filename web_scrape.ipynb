{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Selenium & Beatuiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully recovered 50 athletes for basketball 2024!\n",
      "========== Starting Scrape ===========\n",
      "Error on the 'NIL' page for athlete: Dylan Harper\n",
      "Removing all previous information about this player! (i.e name, exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight, colleges, college_status, college_distance, num_offers)\n",
      "Error on the 'Recruiting' page for athlete: Tre Johnson\n",
      "Removing all previous information about this player! (i.e name, exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight)\n",
      "Error on the 'Player' page for athlete: Jalil Bethea\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Karter Knox\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Asa Newell\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jayden Quaintance\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Derrion Reid\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Carter Bryant\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Bryson Tucker\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Boogie Fland\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Zoom Diallo\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jason Asemota\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Billy Richmond\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Aiden Sherrell\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Rob Wright\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Donnie Freeman\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jamari Phillips\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Ahmad Nowell\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Austin Swartz\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Naas Cunningham\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Morez Johnson\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jackson McAndrew\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Kur Teng\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jalen Shelley\n",
      "Removing all previous information about this player! (i.e name)\n",
      "Error on the 'Player' page for athlete: Jaeden Mustaf\n",
      "Removing all previous information about this player! (i.e name)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/cody/Documents/MADS/SIADS 696/MS2_NIL/web_scrape.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X20sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X20sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     driver\u001b[39m.\u001b[39;49mget(athlete_link)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X20sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:353\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: url})\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m--> 344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[1;32m    345\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=116.0.5845.187)\nStacktrace:\n0   chromedriver                        0x00000001028ea65c chromedriver + 4318812\n1   chromedriver                        0x00000001028e2d00 chromedriver + 4287744\n2   chromedriver                        0x00000001025147ec chromedriver + 296940\n3   chromedriver                        0x00000001024ec6e4 chromedriver + 132836\n4   chromedriver                        0x0000000102577de4 chromedriver + 703972\n5   chromedriver                        0x000000010258a5b8 chromedriver + 779704\n6   chromedriver                        0x0000000102546178 chromedriver + 500088\n7   chromedriver                        0x0000000102546fc0 chromedriver + 503744\n8   chromedriver                        0x00000001028aac40 chromedriver + 4058176\n9   chromedriver                        0x00000001028af160 chromedriver + 4075872\n10  chromedriver                        0x0000000102872e68 chromedriver + 3829352\n11  chromedriver                        0x00000001028afc4c chromedriver + 4078668\n12  chromedriver                        0x0000000102887f08 chromedriver + 3915528\n13  chromedriver                        0x00000001028cc140 chromedriver + 4194624\n14  chromedriver                        0x00000001028cc2c4 chromedriver + 4195012\n15  chromedriver                        0x00000001028dc4d0 chromedriver + 4261072\n16  libsystem_pthread.dylib             0x000000019930bfa8 _pthread_start + 148\n17  libsystem_pthread.dylib             0x0000000199306da0 thread_start + 8\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/cody/Documents/MADS/SIADS 696/MS2_NIL/web_scrape.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X20sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m             pos_height_weight\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mnan)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X20sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X20sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError on the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mPlayer\u001b[39m\u001b[39m'\u001b[39m\u001b[39m page for athlete: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(names[i]))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X20sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRemoving all previous information about this player! (i.e name)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X20sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m     names\u001b[39m.\u001b[39mpop(i)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sports = ['football']\n",
    "#years = ['2022','2023','2024','2025','2026']\n",
    "years = ['2021']\n",
    "\n",
    "exp = []\n",
    "athlete_grade = []\n",
    "pos_height_weight = []\n",
    "ages = []\n",
    "ranks = []\n",
    "high_school = []\n",
    "home_town = []\n",
    "colleges = []\n",
    "college_status = []\n",
    "college_distance = []\n",
    "num_offers = []\n",
    "NIL_val = []\n",
    "instagram_followers = []\n",
    "twitter_followers = []\n",
    "tiktok_followers = []\n",
    "\n",
    "for year in years:\n",
    "    for sport in sports:\n",
    "        \n",
    "        # Selenium Driver to click on \"Load More\"\n",
    "        driver = webdriver.Chrome()\n",
    "        URL = f'https://www.on3.com/db/rankings/industry-player/{sport}/{year}/'\n",
    "        driver.get(URL)\n",
    "\n",
    "        dummyCount = 0\n",
    "        \n",
    "        ###########################################################################################################################\n",
    "        # Click the \"Load More\" button such that all athelete links are visible\n",
    "        # dummyCount was implemented for sports/years in which the number of athletes is very big (ex: football 2023 has 3000+ athletes) AND\n",
    "        # the 'Load More' button does not disappear when pressed to completiion... so a simple counter was implemented\n",
    "        # By making this condition: 'dummyCount < 19' we max out at 1000 athletes\n",
    "        # while (dummyCount < 19):\n",
    "        #     try:\n",
    "        #         load_more_button = WebDriverWait(driver, 10).until(\n",
    "        #             EC.element_to_be_clickable((By.XPATH, \"//span[@class='MuiButton-label' and contains(text(), 'Load More')]\"))\n",
    "        #         )\n",
    "        #         load_more_button.click()\n",
    "        #         time.sleep(10)\n",
    "        #         dummyCount += 1\n",
    "        #     except:\n",
    "        #         break\n",
    "\n",
    "        ###########################################################################################################################\n",
    "        \n",
    "        ###########################################################################################################################\n",
    "        # Get all athlete names & associated links\n",
    "        # New Information: names, links\n",
    "        page_source = driver.page_source\n",
    "        page_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        results = page_soup.find(class_=\"PlayerRankings_playerRankings__jvfFg\")\n",
    "\n",
    "        athletes = results.find_all('a', class_=\"MuiTypography-root MuiLink-root MuiLink-underlineHover MuiTypography-h5 MuiTypography-colorPrimary\")\n",
    "        names = [athlete.text for athlete in athletes]\n",
    "\n",
    "        # Generate a list of links that we can iterate through\n",
    "        links = [athlete['href'] for athlete in athletes]\n",
    "        base_url = \"https://www.on3.com/\"\n",
    "        athlete_links = [urljoin(base_url, link) for link in links]\n",
    "\n",
    "        tot = len(athlete_links)\n",
    "\n",
    "        print('Successfully recovered {} athletes for {} {}!'.format(tot, sport, year))\n",
    "        print('========== Starting Scrape ===========')\n",
    "\n",
    "        ###########################################################################################################################\n",
    "\n",
    "        for i, athlete_link in enumerate(athlete_links):\n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"Player\" page for each of the athletes\n",
    "            # New Information: exp, athlete_grade, ages, high_school, home_town, pos_height_weight\n",
    "            try:\n",
    "                driver.get(athlete_link)\n",
    "                time.sleep(3)\n",
    "\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "                CollegeRankingInfo = page_soup.find(class_='CollegeRanking_info__LM3nn')\n",
    "\n",
    "                if CollegeRankingInfo:\n",
    "                    exp_year = CollegeRankingInfo.find_all(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    if len(exp_year) == 2:\n",
    "                        exp.append(exp_year[0].text)\n",
    "                        athlete_grade.append(exp_year[1].text)\n",
    "                    else:\n",
    "                        exp.append(np.nan)\n",
    "                        athlete_grade.append(np.nan)\n",
    "                    age = CollegeRankingInfo.find(class_='MuiTypography-root CollegeRanking_span__qtAfW MuiTypography-subtitle1 MuiTypography-colorPrimary')\n",
    "                    ages.append(age.text)\n",
    "                else:\n",
    "                    exp.append(np.nan)\n",
    "                    athlete_grade.append(np.nan)\n",
    "                    ages.append(np.nan)\n",
    "                \n",
    "                RecruitModuleInfo = page_soup.find(class_='RecruitModule_info__Ugxqd')\n",
    "\n",
    "                if RecruitModuleInfo:\n",
    "                    # Moving over to \"Recruiting\" page as this is more robust for more athletes\n",
    "                    # ranking = RecruitModuleInfo.find(class_='RecruitModule_rating__sONqb')\n",
    "                    # ranks.append(ranking.text)\n",
    "\n",
    "                    homeInfo = RecruitModuleInfo.find_all(class_='MuiTypography-root RecruitModule_span__KmmzN MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                    high_school.append(homeInfo[-2].text)\n",
    "                    home_town.append(homeInfo[-1].text)\n",
    "                else:\n",
    "                    # ranks.append(np.nan)\n",
    "                    high_school.append(np.nan)\n",
    "                    home_town.append(np.nan)\n",
    "\n",
    "                Attributes = page_soup.find(class_='MeasurementInfo_info__IHmGD')\n",
    "\n",
    "                if Attributes:\n",
    "                    dummy = Attributes.find_all(class_='MuiTypography-root MeasurementInfo_text__dCryI MuiTypography-body1 MuiTypography-colorTextPrimary')\n",
    "                    if len(dummy) >= 2:\n",
    "                        pos_height_weight.append(dummy[1].text)\n",
    "                    else:\n",
    "                        pos_height_weight.append(np.nan)\n",
    "            \n",
    "            except:\n",
    "                print(\"Error on the 'Player' page for athlete: {}\".format(names[i]))\n",
    "                print(\"Removing all previous information about this player! (i.e name)\")\n",
    "                names.pop(i)\n",
    "                continue\n",
    "            ###########################################################################################################################\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"Recruiting\" page for each of the athletes\n",
    "            # New Information: ranks, colleges (the college this athlete is targeting), college_status, college_distance, num_offers\n",
    "            try:\n",
    "                driver.get(urljoin(athlete_link,'recruiting/'))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "                Ranks = page_soup.find(class_=\"Rankings_industryRankWrapper__2qwnq\")\n",
    "\n",
    "                if Ranks:\n",
    "                    ranking = Ranks.find(class_=\"MuiTypography-root Rankings_industryRating__9uavm MuiTypography-body1 MuiTypography-colorTextPrimary\")\n",
    "                    ranks.append(ranking.text)\n",
    "                else:\n",
    "                    ranks.append(np.nan)\n",
    "                \n",
    "                all_team_link = page_soup.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineHover PlayerInterestsModule_text__kjqNU MuiTypography-caption MuiTypography-colorPrimary')\n",
    "                driver.get(urljoin(base_url,all_team_link['href']))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "\n",
    "                RecruitColleges = page_soup.find_all(class_='PlayerInterestsItem_teamContainer__vjQkf')\n",
    "                \n",
    "                if RecruitColleges:\n",
    "                    dummy_colleges=[]\n",
    "                    count = 0\n",
    "                \n",
    "                    for college in RecruitColleges:\n",
    "                        college_name = college.find(class_='MuiTypography-root MuiLink-root MuiLink-underlineNone PlayerInterestsItem_teamName__FeBHv MuiTypography-h5 MuiTypography-colorPrimary')\n",
    "                        if year == '2024' or year == '2025' or year == '2026':\n",
    "                            college_statuses = college.find(class_='MuiTypography-root PlayerInterestsItem_status__1_rA8 PlayerInterestsItem_offered__OxPV0 MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                        else:\n",
    "                            college_statuses = college.find(class_='MuiTypography-root PlayerInterestsItem_status__1_rA8 MuiTypography-subtitle1 MuiTypography-colorTextPrimary')\n",
    "                        college_dist = college.find(class_='MuiTypography-root PlayerInterestsItem_distanceText__KJhj3 MuiTypography-caption MuiTypography-colorTextPrimary')\n",
    "                        if count == 0:\n",
    "                            colleges.append(college_name.text)\n",
    "                            college_status.append(college_statuses.text)\n",
    "                            college_distance.append(college_dist.text)\n",
    "                        count +=1\n",
    "                    num_offers.append(count)\n",
    "                else:\n",
    "                    colleges.append(np.nan)\n",
    "                    college_status.append(np.nan)\n",
    "                    college_distance.append(np.nan)\n",
    "                    num_offers.append(np.nan)\n",
    "                    \n",
    "            except:\n",
    "                print(\"Error on the 'Recruiting' page for athlete: {}\".format(names[i]))\n",
    "                print(\"Removing all previous information about this player! (i.e name, exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight)\")\n",
    "                names.pop(i)\n",
    "                exp.pop()\n",
    "                athlete_grade.pop()\n",
    "                ages.pop()\n",
    "                high_school.pop()\n",
    "                home_town.pop()\n",
    "                pos_height_weight.pop()\n",
    "                continue\n",
    "            ###########################################################################################################################\n",
    "            \n",
    "            ###########################################################################################################################\n",
    "            # Information from the \"NIL\" page for each of the athletes\n",
    "            # New Information: NIL_val, instagram_followers, twitter_followers, tiktok_followers\n",
    "            try:\n",
    "                driver.get(urljoin(athlete_link,'nil/'))\n",
    "                time.sleep(3)\n",
    "                athlete_source = driver.page_source\n",
    "                page_soup = BeautifulSoup(athlete_source, 'html.parser')\n",
    "                \n",
    "                RecruitNIL = page_soup.find(class_='NilValuationCircle_nilCircleValue__wzomB')\n",
    "\n",
    "                if RecruitNIL:\n",
    "                    NIL_val.append(RecruitNIL.text)\n",
    "                else:\n",
    "                    NIL_val.append(np.nan)\n",
    "                \n",
    "                RecruitSocials = page_soup.find(class_=\"NilSocialValuations_socialValuations__MeR7O\")\n",
    "\n",
    "                instagram = np.nan\n",
    "                twitter = np.nan\n",
    "                tiktok = np.nan\n",
    "\n",
    "                if RecruitSocials:\n",
    "                    socials = RecruitSocials.find_all(class_=\"NilSocialValuations_platform__3qBMy\")\n",
    "                    for social in socials:\n",
    "                        social_name = social.find('a')['href']\n",
    "                        if 'instagram' in social_name:\n",
    "                            instagram = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'twitter' in social_name:\n",
    "                            twitter = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                        elif 'tiktok' in social_name:\n",
    "                            tiktok = social.find(class_='MuiTypography-root NilSocialValuations_platformFollowers__sW8kK MuiTypography-body1 MuiTypography-colorTextPrimary').text\n",
    "                \n",
    "                instagram_followers.append(instagram)\n",
    "                twitter_followers.append(twitter)\n",
    "                tiktok_followers.append(tiktok)\n",
    "                \n",
    "            except:\n",
    "                print(\"Error on the 'NIL' page for athlete: {}\".format(names[i]))\n",
    "                print(\"Removing all previous information about this player! (i.e name, exp, athlete_grade, ages, ranks, high_school, home_town, pos_height_weight, colleges, college_status, college_distance, num_offers)\")\n",
    "                names.pop(i)\n",
    "                exp.pop()\n",
    "                athlete_grade.pop()\n",
    "                ages.pop()\n",
    "                ranks.pop()\n",
    "                high_school.pop()\n",
    "                home_town.pop()\n",
    "                pos_height_weight.pop()\n",
    "                colleges.pop()\n",
    "                college_status.pop()\n",
    "                college_distance.pop()\n",
    "                num_offers.pop()\n",
    "                continue\n",
    "            ###########################################################################################################################\n",
    "\n",
    "            ###########################################################################################################################\n",
    "            # Print something out to the user, like a progress bar\n",
    "            if (i % 10 == 0) and (i != 0):\n",
    "                print(\"=== Scraped {:.2f} % of the Players ===\".format((i/tot)*100))\n",
    "                print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "                      len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),\n",
    "                      len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "                print(\"======================================\")\n",
    "            ###########################################################################################################################\n",
    "        \n",
    "        print(\"=== Scraped 100.0 % of the Players ===\")\n",
    "        print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "              len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),\n",
    "              len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "        print(\"======================================\")\n",
    "        print('Successfully completed {} athletes for {} {}!'.format(len(names), sport, year))\n",
    "        print('Number of removed athletes: {}'.format(len(athlete_links) - len(names)))\n",
    "        #end\n",
    "    #end\n",
    "#end\n",
    "\n",
    "# Close the instance of the webpage\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/db/dylan-harper-150969/recruiting/165435/interests/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_team_link['href']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 1 1 1 1 2 1 1 2 0 0 0 0 0 0 0\n",
      "Dylan Harper nan CG / 6-6 / 205 nan nan 99.70+ nan\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/cody/Documents/MADS/SIADS 696/MS2_NIL/web_scrape.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(names), \u001b[39mlen\u001b[39m(exp), \u001b[39mlen\u001b[39m(pos_height_weight), \u001b[39mlen\u001b[39m(athlete_grade), \u001b[39mlen\u001b[39m(ages), \u001b[39mlen\u001b[39m(ranks), \u001b[39mlen\u001b[39m(high_school), \u001b[39mlen\u001b[39m(home_town),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m       \u001b[39mlen\u001b[39m(colleges), \u001b[39mlen\u001b[39m(college_status), \u001b[39mlen\u001b[39m(college_distance), \u001b[39mlen\u001b[39m(num_offers), \u001b[39mlen\u001b[39m(NIL_val),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m       \u001b[39mlen\u001b[39m(instagram_followers), \u001b[39mlen\u001b[39m(twitter_followers),\u001b[39mlen\u001b[39m(tiktok_followers))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(names[\u001b[39m0\u001b[39m], exp[\u001b[39m0\u001b[39m], pos_height_weight[\u001b[39m0\u001b[39m], athlete_grade[\u001b[39m0\u001b[39m], ages[\u001b[39m0\u001b[39m], ranks[\u001b[39m0\u001b[39m], high_school[\u001b[39m0\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(home_town[\u001b[39m0\u001b[39m], colleges[\u001b[39m0\u001b[39m], college_status[\u001b[39m0\u001b[39;49m], college_distance[\u001b[39m0\u001b[39m], num_offers[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cody/Documents/MADS/SIADS%20696/MS2_NIL/web_scrape.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# print(NIL_val[0], instagram_followers[0], twitter_followers[0], tiktok_followers[0])\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(len(names), len(exp), len(pos_height_weight), len(athlete_grade), len(ages), len(ranks), len(high_school), len(home_town),\n",
    "      len(colleges), len(college_status), len(college_distance), len(num_offers), len(NIL_val),\n",
    "      len(instagram_followers), len(twitter_followers),len(tiktok_followers))\n",
    "#print(names[0], exp[0], pos_height_weight[0], athlete_grade[0], ages[0], ranks[0], high_school[0])\n",
    "#print(home_town[0], colleges[0], college_status[0], college_distance[0], num_offers[0])\n",
    "# print(NIL_val[0], instagram_followers[0], twitter_followers[0], tiktok_followers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv saved to 'csv_files' folder!'\n"
     ]
    }
   ],
   "source": [
    "column_names = ['NAME', 'EXP', 'POS_HEI_WEI', 'GRADE', 'AGE', 'SKILL', 'HISCH', 'HOTOWN', 'STARCOLL', 'STARCOLLSTAT', 'COLLDIST', 'NUMOFF', 'INSTA', 'TWIT', 'TIK', 'NILVAL']\n",
    "\n",
    "csv_file_path = 'csv_files/{}_{}.csv'.format(sport, year)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    \n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    for row in zip(names, exp, pos_height_weight, athlete_grade, ages, ranks, high_school, home_town, colleges, college_status, college_distance, num_offers, instagram_followers, twitter_followers, tiktok_followers, NIL_val):\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"csv saved to 'csv_files' folder!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
